## Introduction to  Spark


### 1. Spark框架的概述

- **Spark定义**: Apache Spark是用于大规模数据(large-scala data)处理的统一(unified)分析引擎。
- **Spark起源**: Spark最早源于一篇论文 *Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing*, 论文中提出了一种弹性分布式数据集(RDD)的概念。
- **RDD**: RDD是一种分布式内存抽象,其使得程序员能够在大规模集群中做**内存运算**,并且有一定的**容错**方式.而这也是整个Spark的核心数据结构,Spark整个平台都围绕着RDD进行.


### 1. 环境搭建


### 2. 入门概念