{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PySpark\n",
    "\n",
    "## 1. PySpark 核心类介绍\n",
    "\n",
    "- **pyspark.SparkContext** : SparkContext存在于 **Driver** 中, 是Spark库的主要入口点, 它表示与Spark集群的一个连接, 其他重要的对象都要依赖它. 它可以在集群上创建RDD, accumulators和广播变量等分布式数据结构.\n",
    "- **pyspark.sql.SparkSession** : SparkSession是Spark 2.0中引入的新的入口点, 它是Spark SQL, DataFrame和Dataset API的入口点. `SparkSession`是创建DataFrame和执行SQL查询的主要入口点.\n",
    "- **pyspark.sql.DataFrame** : 是Spark SQL的主要抽象对象, 若干行的分布式数据, 每一行都有若干个有名字的列.  跟R/Python中的DataFrame 相像 , 有着更丰富的优化. DataFrame可以有很多种方式进行构造, 例如:  结构化数据文件, Hive的table, 外部数据库, RDD.\n",
    "- **pyspark.sql.Column** :  DataFrame 的列表达.\n",
    "- **pyspark.sql.Row** :  DataFrame 的行数据.\n",
    "\n",
    "## 2. `pyspark.SparkContext` & `pyspark.sql.SparkSession`\n",
    "\n",
    "Spark 2.20 以后 SparkSession 合并了 SQLContext 和 HiveContext, 同时支持Hive, 包括HIveSOL, Hive UDFs 的入口, 以及从Hive table中读取数据."
   ],
   "id": "5d3a3126f51ac3d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:50:51.592206Z",
     "start_time": "2025-07-21T08:50:51.574157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = \"C:/Program Files/Java/jdk-1.8\"\n",
    "# 配置hadoop路径\n",
    "os.environ['HADOOP_HOME'] = \"D:/hadoop-3.3.4\"\n",
    "# 配置python解释器\n",
    "os.environ['PYSPARK_PYTHON'] = \"D:/Anaconda/envs/dsml/python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"D:/Anaconda/envs/dsml/python.exe\""
   ],
   "id": "d2d18bc96cf1bddd",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:50:57.689340Z",
     "start_time": "2025-07-21T08:50:57.484543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "## 获取或者新建一个 sparkSession\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Python Spark SQL basic example\")\n",
    "         .master(\"local[2]\")       #spark master URL. 本地为local, “local[4]” 本地4核,\n",
    "                                   # or “spark://master:7077” to run on a Spark standalone cluster\n",
    "         .config(\"spark.executor.memory\", \"512m\")\n",
    "         .getOrCreate() )          # 获取现有的SparkSession或创建新的，避免重复创建\n",
    "\n",
    "## 从SparkSession中获取底层的SparkContext对象\n",
    "## SparkContext是Spark功能的原始入口点，用于:\n",
    "#    创建RDDs\n",
    "#    设置配置\n",
    "#    访问集群资源\n",
    "#    低级API操作\n",
    "sc = spark.sparkContext\n"
   ],
   "id": "aa894d067504cc0a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. `pyspark.sql.DataFrame` : DataFrame\n",
    "\n",
    "### 3.1. 创建DataFrame\n",
    "有了SparkSession, DataFrame可以从已有的RDD, Hive table, 或者其他spark的数据源进行创建."
   ],
   "id": "7450a9eebf171ceb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:51:02.834924Z",
     "start_time": "2025-07-21T08:51:02.498451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# spark is an existing SparkSession\n",
    "# 从文件读取\n",
    "## read.json\n",
    "df = spark.read.json(\"C:\\\\Users\\\\86188\\\\Desktop\\\\CODE\\\\llm-from-scratch\\\\data_aug.jsonl\")\n",
    "df.show()\n",
    "# df = spark.read.load(\"C:\\\\Users\\\\86188\\\\Desktop\\\\CODE\\\\llm-from-scratch\\\\data_aug.jsonl\", format=\"json\") #format: Default to 'parquet'\n",
    "\n",
    "## read.csv\n",
    "## df_csv = spark.read.csv(\"examples/src/main/resources/people.csv\",sep=';', header= True)\n",
    "## read.text\n",
    "## df_txt = spark.read.text(\"examples/src/main/resources/people.txt\")\n",
    "## read.parquet\n",
    "## df_parquet = spark.read.parquet(\"examples/src/main/resources/users.parquet\")\n",
    "## rdd\n",
    "#  sc = spark.sparkContext\n",
    "#  rdd = sc.textFile('examples/src/main/resources/people.json')\n",
    "#  df_rdd1 = spark.read.json(rdd)"
   ],
   "id": "aa7fcdf55dadc465",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------------+-------------------------------------+\n",
      "|                 input|           instruction|                               output|\n",
      "+----------------------+----------------------+-------------------------------------+\n",
      "|今天北京的天气怎么样？|今天北京的天气怎么样？|         北京今天晴，气温在25度左右。|\n",
      "|如何提高英语口语能力？|如何提高英语口语能力？|可以通过多听多说、多与人交流、模仿...|\n",
      "+----------------------+----------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:51:14.800461Z",
     "start_time": "2025-07-21T08:51:12.819810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 通过 python list 创建DataFrame\n",
    "# [\"name\", \"height\"] 指定列名为 name 和 height\n",
    "df_list = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
    "df_list.show()"
   ],
   "id": "9127b2ca7419d90d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|  Tom|    80|\n",
      "|Alice|  NULL|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:51:21.263129Z",
     "start_time": "2025-07-21T08:51:18.779820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 通过RDD创建DataFrame, 更推荐\n",
    "rdd_list = [('Alice', 1), ('Bob', 2)]\n",
    "rdd = sc.parallelize(rdd_list)  # sc.parallelize(l)将Python列表转换为Spark的RDD\n",
    "df_rdd = spark.createDataFrame(rdd,['name', 'age'])\n",
    "df_rdd.show()"
   ],
   "id": "3ced5b4cc3ccbf10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:51:31.568035Z",
     "start_time": "2025-07-21T08:51:29.864034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 通过定义Schema创建DataFrame\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "schema = StructType([    # StructType: 定义表结构 (类似关系型数据库的表结构)\n",
    "    StructField(\"name\", StringType(), True),  # StructField: 定义每个列的属性：\n",
    "    StructField(\"age\", IntegerType(), True)])\n",
    "# 使用之前创建的 rdd 和定义好的schema创建DataFrame\n",
    "df_schema = spark.createDataFrame(rdd, schema)\n",
    "df_schema.show()"
   ],
   "id": "563a3f70c4daf566",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:51:37.947076Z",
     "start_time": "2025-07-21T08:51:36.251676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 从Pandas DataFrame 创建 spark DataFrame\n",
    "import pandas as pd\n",
    "df_pandas = spark.createDataFrame(pd.DataFrame([(\"a\", 6), (\"b\", 7)], columns=[\"name\", \"age\"]))\n",
    "df_pandas.show()"
   ],
   "id": "a453cf04dc70f3fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|   a|  6|\n",
      "|   b|  7|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2. DataFrame 常用方法\n",
    "\n",
    "关于DataFrame的操作, 感觉上和`pandas.DataFrame`的操作很类似, 很多时候都可以触类旁通.\n",
    "\n",
    "<p>\n",
    "\n",
    "Spark 的操作分为两部分, 转换(transformation) 和 执行 (action). 操作是**lazy**模式, 只有遇到 **执行操作(action)** 才会执行\n",
    "\n",
    "<p>\n",
    "\n",
    "基本操作:\n",
    "\n",
    "```python\n",
    "df_customers.cache() # 以列式存储在内存中\n",
    "df_customers.persist() # 缓存到内存中\n",
    "df_customers.unpersist() # 移除所有的blocks\n",
    "df_customers.coalesce(numPartitions= 1) #返回一个有着numPartition的DataFrame\n",
    "df_customers.repartition(10) ##repartitonByRange\n",
    "df_customers.rdd.getNumPartitions()# 查看 partitons 个数\n",
    "df_customers.columns # 查看列名\n",
    "df_customers.dtypes # 返回列的数据类型\n",
    "df_customers.explain() #返回物理计划，调试时应用\n",
    "```\n"
   ],
   "id": "cfcac06ec96e9edb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T09:00:01.478989Z",
     "start_time": "2025-07-21T08:59:59.675973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 创建DataFrame : customers, products, sales\n",
    "customers =  [(1,'James',21,'M'), (2, \"Liz\",25,\"F\"), (3, \"John\", 31, \"M\"),\n",
    "     (4, \"Jennifer\", 45, \"F\"), (5, \"Robert\", 41, \"M\"), (6, \"Sandra\", 45, \"F\")]\n",
    "df_customers = spark.createDataFrame(customers, [\"cID\", \"name\", \"age\", \"gender\"]) # list -> DF\n",
    "df_customers.show()"
   ],
   "id": "3676bc7fc3ce3d29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+\n",
      "|cID|    name|age|gender|\n",
      "+---+--------+---+------+\n",
      "|  1|   James| 21|     M|\n",
      "|  2|     Liz| 25|     F|\n",
      "|  3|    John| 31|     M|\n",
      "|  4|Jennifer| 45|     F|\n",
      "|  5|  Robert| 41|     M|\n",
      "|  6|  Sandra| 45|     F|\n",
      "+---+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T09:00:07.772974Z",
     "start_time": "2025-07-21T09:00:05.343625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "products = [(1, \"iPhone\", 600, 400), (2, \"Galaxy\", 500, 400), (3, \"iPad\", 400, 300),\n",
    "            (4, \"Kindel\",200,100), (5, \"MacBook\", 1200, 900), (6, \"Dell\",500, 400)]\n",
    "df_products = sc.parallelize(products).toDF([\"pId\", \"name\", \"price\", \"cost\"]) # List-> RDD ->DF\n",
    "df_products.show()"
   ],
   "id": "72f85973c82d9eaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+----+\n",
      "|pId|   name|price|cost|\n",
      "+---+-------+-----+----+\n",
      "|  1| iPhone|  600| 400|\n",
      "|  2| Galaxy|  500| 400|\n",
      "|  3|   iPad|  400| 300|\n",
      "|  4| Kindel|  200| 100|\n",
      "|  5|MacBook| 1200| 900|\n",
      "|  6|   Dell|  500| 400|\n",
      "+---+-------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T09:31:08.945136Z",
     "start_time": "2025-07-21T09:31:08.223990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sales = [(\"01/01/2015\", \"iPhone\", \"USA\", 40000), (\"01/02/2015\", \"iPhone\", \"USA\", 30000),\n",
    "        (\"01/02/2015\", \"iPhone\", \"China\", 10000), (\"01/02/2015\", \"iPhone\", \"China\", 5000),\n",
    "        (\"01/01/2015\", \"S6\", \"USA\", 20000), (\"01/02/2015\", \"S6\", \"USA\", 10000),\n",
    "        (\"01/01/2015\", \"S6\", \"China\", 9000), (\"01/02/2015\", \"S6\", \"China\", 6000)]\n",
    "df_sales = spark.createDataFrame(sales, [\"date\", \"product\", \"country\", \"revenue\"])"
   ],
   "id": "abdc83308db7e2d7",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m sales \u001B[38;5;241m=\u001B[39m [(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m01/01/2015\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miPhone\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUSA\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m40000\u001B[39m), (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m01/02/2015\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miPhone\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUSA\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m30000\u001B[39m),\n\u001B[0;32m      2\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m01/02/2015\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miPhone\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChina\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m10000\u001B[39m), (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m01/02/2015\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miPhone\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChina\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m5000\u001B[39m),\n\u001B[0;32m      3\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m01/01/2015\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mS6\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUSA\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m20000\u001B[39m), (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m01/02/2015\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mS6\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUSA\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m10000\u001B[39m),\n\u001B[0;32m      4\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m01/01/2015\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mS6\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChina\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m9000\u001B[39m), (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m01/02/2015\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mS6\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChina\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m6000\u001B[39m)]\n\u001B[1;32m----> 5\u001B[0m df_sales \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43msales\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdate\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mproduct\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcountry\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrevenue\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\dsml\\lib\\site-packages\\pyspark\\sql\\session.py:1443\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[1;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[0;32m   1438\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[0;32m   1439\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[0;32m   1440\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[0;32m   1441\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[0;32m   1442\u001B[0m     )\n\u001B[1;32m-> 1443\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1444\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m   1445\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\dsml\\lib\\site-packages\\pyspark\\sql\\session.py:1485\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[1;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[0;32m   1483\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[0;32m   1484\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1485\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromLocal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1486\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1487\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\dsml\\lib\\site-packages\\pyspark\\sql\\session.py:1116\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[1;34m(self, data, schema)\u001B[0m\n\u001B[0;32m   1114\u001B[0m \u001B[38;5;66;03m# convert python objects to sql data\u001B[39;00m\n\u001B[0;32m   1115\u001B[0m internal_data \u001B[38;5;241m=\u001B[39m [struct\u001B[38;5;241m.\u001B[39mtoInternal(row) \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m tupled_data]\n\u001B[1;32m-> 1116\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparallelize\u001B[49m\u001B[43m(\u001B[49m\u001B[43minternal_data\u001B[49m\u001B[43m)\u001B[49m, struct\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\dsml\\lib\\site-packages\\pyspark\\context.py:783\u001B[0m, in \u001B[0;36mSparkContext.parallelize\u001B[1;34m(self, c, numSlices)\u001B[0m\n\u001B[0;32m    751\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mparallelize\u001B[39m(\u001B[38;5;28mself\u001B[39m, c: Iterable[T], numSlices: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RDD[T]:\n\u001B[0;32m    752\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    753\u001B[0m \u001B[38;5;124;03m    Distribute a local Python collection to form an RDD. Using range\u001B[39;00m\n\u001B[0;32m    754\u001B[0m \u001B[38;5;124;03m    is recommended if the input represents a range for performance.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    781\u001B[0m \u001B[38;5;124;03m    [['a'], ['b', 'c']]\u001B[39;00m\n\u001B[0;32m    782\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 783\u001B[0m     numSlices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(numSlices) \u001B[38;5;28;01mif\u001B[39;00m numSlices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdefaultParallelism\u001B[49m\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mrange\u001B[39m):\n\u001B[0;32m    785\u001B[0m         size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(c)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\dsml\\lib\\site-packages\\pyspark\\context.py:630\u001B[0m, in \u001B[0;36mSparkContext.defaultParallelism\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    618\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    619\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdefaultParallelism\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m    620\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    621\u001B[0m \u001B[38;5;124;03m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001B[39;00m\n\u001B[0;32m    622\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;124;03m    True\u001B[39;00m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msc\u001B[49m()\u001B[38;5;241m.\u001B[39mdefaultParallelism()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T09:02:38.938356Z",
     "start_time": "2025-07-21T09:02:38.893027Z"
    }
   },
   "cell_type": "code",
   "source": "df_customers.rdd.getNumPartitions()# 查看 partitons 个数",
   "id": "e7d365f384723aa5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.2.1 Actions\n",
    "\n",
    "- Action 操作会触发实际的计算并返回结果 (到Driver程序或存储系统). 执行Action时, Spark会执行之前所有的Transformation操作(惰性求值).\n",
    "- 减少Action操作: 避免频繁调用`collect()`等操作, 减少Driver内存压力\n",
    "- Actions 特点:\n",
    "    - 立即触发计算\n",
    "    - 通常返回非DataFrame结果 (标量、数组或直接输出)\n",
    "    - 有些操作可能将大量数据传输到 Driver (如collect())\n",
    "\n",
    "- Common Actions\n",
    "\n",
    "| 操作             | 描述 | 示例 |\n",
    "|----------------|--|------|\n",
    "| `show()`       | 显示DataFrame内容 | `df.show(10)` |\n",
    "| `collect()`    | 返回所有数据到Driver | `df.collect()` |\n",
    "| `count()`      | 返回行数 | `df.count()` |\n",
    "| `first()`      | 返回第一行 | `df.first()` |\n",
    "| `take(n)`      | 返回前n行 | `df.take(5)` |\n",
    "| `head(n)`      | 同take | `df.head(5)` |\n",
    "| `foreach()`      | 对每行应用函数 | `df.foreach(print)` |\n",
    "| `toPandas()`      | 转为Pandas DataFrame | `df.limit(1000).toPandas()` |\n",
    "| `write.save()` | 保存DataFrame | `df.write.csv(\"path\")` |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "3328579d58f42a3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.2.2 Transformations\n",
    "\n",
    "Transformation 操作只是定义计算逻辑, 不会立即执行, 而是记录在\"执行计划\"中, 直到遇到Action才执行.\n",
    "\n",
    "- 常规Transformation操作\n",
    "\n",
    "| 操作              | 描述 | 示例                               |\n",
    "|-----------------|------|----------------------------------|\n",
    "| `select()`        | 选择列 | `df.select(\"name\", \"age\")` |\n",
    "| `filter()/where()` | 条件过滤 | `df.filter(df.age > 30)` |\n",
    "| `withColumn()`    | 添加/替换列 | `df.withColumn(\"age2\", df.age+1)` |\n",
    "| `drop()`          | 删除列 | `df.drop(\"temp_col\")`            |\n",
    "| `limit()`         | 限制行数 | `df.limit(100)`                  |\n",
    "\n",
    "- 聚合操作\n",
    "\n",
    "| 操作方法        | 功能描述                 | 示例代码                                      | 说明                              |\n",
    "|----------------|--------------------------|---------------------------------------------|---------------------------------|\n",
    "| `groupBy()`     | 基础分组操作              | `df.groupBy(\"dept\")`                        | 按指定列分组，返回GroupedData对象，需配合聚合函数使用 |\n",
    "| `agg()`         | 聚合计算                  | `df.groupBy().agg({\"salary\":\"avg\"})`        | 可对分组后的数据执行多种聚合计算，支持字典或表达式形式     |\n",
    "| `rollup()`      | 多维聚合（层次式）        | `df.rollup(\"region\").count()`               | 生成从最细粒度到总计的多层次聚合(如地区小计→全国总计)    |\n",
    "| `cube()`        | 全组合多维聚合            | `df.cube(\"year\", \"month\").sum()`            | 生成所有列组合的聚合                      |\n",
    "\n",
    "| 聚合函数       | 描述                  | 使用示例                                      |\n",
    "|---------------|-----------------------|---------------------------------------------|\n",
    "| `count()`     | 计数                  | `df.groupBy(\"dept\").count()`                |\n",
    "| `avg()`       | 平均值                | `df.groupBy().agg(F.avg(\"salary\"))`         |\n",
    "| `sum()`       | 求和                  | `df.rollup(\"region\").sum(\"sales\")`          |\n",
    "| `max()`       | 最大值                | `df.cube(\"year\").max(\"temperature\")`        |\n",
    "| `min()`       | 最小值                | `df.groupBy(\"team\").min(\"score\")`           |\n",
    "| `collect_list()` | 收集为列表           | `df.groupBy(\"class\").agg(F.collect_list(\"name\"))` |\n"
   ],
   "id": "83286d6ac63603d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T09:30:59.777261Z",
     "start_time": "2025-07-21T09:30:58.983095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ],
   "id": "16043988e42ebd91",
   "outputs": [],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
