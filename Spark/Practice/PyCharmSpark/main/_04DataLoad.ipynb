{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Reading CSV data\n",
    "#### 1.1 Reading CSV data with an inferred schema"
   ],
   "id": "e806f80370a761b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:30:02.695432Z",
     "start_time": "2025-04-15T15:30:02.576158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = \"C:/Program Files/Java/jdk-1.8\"\n",
    "# 配置hadoop路径\n",
    "os.environ['HADOOP_HOME'] = \"D:/hadoop-3.3.4\"\n",
    "# 配置python解释器\n",
    "os.environ['PYSPARK_PYTHON'] = \"D:/Anaconda/envs/DDL/python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"D:/Anaconda/envs/DDL/python.exe\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"DataLoad\")\n",
    "         .master(\"local[2]\")\n",
    "         .config(\"spark.executor.memory\", \"512m\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Set log level to ERROR\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "# 获取 SparkContext\n",
    "sc = spark.sparkContext\n",
    "print(spark)\n",
    "print(sc)"
   ],
   "id": "5f807c65740539b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x00000168939D9C40>\n",
      "<SparkContext master=local[2] appName=DataLoad>\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T14:22:41.566173Z",
     "start_time": "2025-04-15T14:22:41.324431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csv_file_location = \"../data/dataload/netflix_titles.csv\"\n",
    "\n",
    "# Read CSV file into a DataFrame\n",
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load(csv_file_location))\n",
    "# Display contents of DataFrame\n",
    "df.show(5)\n"
   ],
   "id": "b4459414c1ed9fd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+---------------+--------------------+-------------+------------------+------------+------+---------+--------------------+--------------------+\n",
      "|show_id|   type|               title|       director|                cast|      country|        date_added|release_year|rating| duration|           listed_in|         description|\n",
      "+-------+-------+--------------------+---------------+--------------------+-------------+------------------+------------+------+---------+--------------------+--------------------+\n",
      "|     s1|  Movie|Dick Johnson Is Dead|Kirsten Johnson|                null|United States|September 25, 2021|        2020| PG-13|   90 min|       Documentaries|As her father nea...|\n",
      "|     s2|TV Show|       Blood & Water|           null|Ama Qamata, Khosi...| South Africa|September 24, 2021|        2021| TV-MA|2 Seasons|International TV ...|After crossing pa...|\n",
      "|     s3|TV Show|           Ganglands|Julien Leclercq|Sami Bouajila, Tr...|         null|September 24, 2021|        2021| TV-MA| 1 Season|Crime TV Shows, I...|To protect his fa...|\n",
      "|     s4|TV Show|Jailbirds New Orl...|           null|                null|         null|September 24, 2021|        2021| TV-MA| 1 Season|Docuseries, Reali...|Feuds, flirtation...|\n",
      "|     s5|TV Show|        Kota Factory|           null|Mayur More, Jiten...|        India|September 24, 2021|        2021| TV-MA|2 Seasons|International TV ...|In a city of coac...|\n",
      "+-------+-------+--------------------+---------------+--------------------+-------------+------------------+------------+------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Print schema of DataFrame\n",
    "df.printSchema()\n"
   ],
   "id": "58f09c91a48609ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.2. Reading CSV data with explicit schema",
   "id": "73fba9ded851442b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:01:41.282640Z",
     "start_time": "2025-04-15T15:01:41.186448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "# Define a Schema\n",
    "schema = StructType([\n",
    "    StructField(\"show_id\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"director\", StringType(), True),\n",
    "    StructField(\"cast\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"date_added\", DateType(), True),\n",
    "    StructField(\"release_year\", IntegerType(), True),\n",
    "    StructField(\"rating\", StringType(), True),\n",
    "    StructField(\"duration\", StringType(), True),\n",
    "    StructField(\"listed_in\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True)])\n",
    "\n",
    "# Read CSV file into a DataFrame\n",
    "df1 = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .schema(schema)\n",
    "      .load(csv_file_location))\n",
    "\n",
    "df1.show(5)\n"
   ],
   "id": "277765758e2824bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+---------------+--------------------+-------------+----------+------------+------+---------+--------------------+--------------------+\n",
      "|show_id|   type|               title|       director|                cast|      country|date_added|release_year|rating| duration|           listed_in|         description|\n",
      "+-------+-------+--------------------+---------------+--------------------+-------------+----------+------------+------+---------+--------------------+--------------------+\n",
      "|     s1|  Movie|Dick Johnson Is Dead|Kirsten Johnson|                null|United States|      null|        2020| PG-13|   90 min|       Documentaries|As her father nea...|\n",
      "|     s2|TV Show|       Blood & Water|           null|Ama Qamata, Khosi...| South Africa|      null|        2021| TV-MA|2 Seasons|International TV ...|After crossing pa...|\n",
      "|     s3|TV Show|           Ganglands|Julien Leclercq|Sami Bouajila, Tr...|         null|      null|        2021| TV-MA| 1 Season|Crime TV Shows, I...|To protect his fa...|\n",
      "|     s4|TV Show|Jailbirds New Orl...|           null|                null|         null|      null|        2021| TV-MA| 1 Season|Docuseries, Reali...|Feuds, flirtation...|\n",
      "|     s5|TV Show|        Kota Factory|           null|Mayur More, Jiten...|        India|      null|        2021| TV-MA|2 Seasons|International TV ...|In a city of coac...|\n",
      "+-------+-------+--------------------+---------------+--------------------+-------------+----------+------------+------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Reading Json data\n",
    "#### 2.1 what is json\n",
    "\n",
    "- JSON文件作为一种轻量级的文本格式, 用于表示结构化数据. JSON 文件通常以 .json 作为文件扩展名, 例如 data.json\n",
    "#### Basic Structure\n",
    "- 一个 JSON 文件通常由一个 **JSON 对象**或一个 **JSON 数组**组成. JSON 对象是一组**无序**的键值对, 而 JSON 数组是一组**有序**的值\n",
    "- JSON对象: `{ \"name\": \"John\", \"age\": 30, \"city\": \"New York\" }`\n",
    "- JSON 数组: `[ \"apple\", \"banana\", \"orange\" ]`\n",
    "- JSON 允许在对象中嵌套对象, 也允许在数组中嵌套数组或对象\n",
    "\n",
    "   ```\n",
    "   { \"person\": { \"name\": \"Alice\", \"age\": 25,\n",
    "   \"address\": { \"city\": \"Paris\", \"zipCode\": \"75001\" } },\n",
    "   \"fruits\": [\"apple\", \"orange\", {\"type\": \"banana\", \"color\": \"yellow\"}] }\n",
    "   ```\n",
    "\n",
    "#### 2.2. Reading JSON data with an inferred schema\n"
   ],
   "id": "2c12d4414b2a162b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T14:41:12.407588Z",
     "start_time": "2025-04-15T14:41:12.328951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_file_location = \"../data/dataload/nobel_prizes.json\"\n",
    "\n",
    "# Read JSON file into a DataFrame\n",
    "df2 = (spark.read.format(\"json\")\n",
    "      .option(\"multiLine\", \"true\")\n",
    "      .load(json_file_location))\n",
    "\n",
    "df2.printSchema()"
   ],
   "id": "133eb71bdca7f3d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- laureates: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- firstname: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- motivation: string (nullable = true)\n",
      " |    |    |-- share: string (nullable = true)\n",
      " |    |    |-- surname: string (nullable = true)\n",
      " |-- overallMotivation: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:01:58.969761Z",
     "start_time": "2025-04-15T15:01:58.904618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display contents of DataFrame\n",
    "df2.show(5)"
   ],
   "id": "cf8b89c44572f48e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+----+\n",
      "|  category|           laureates|overallMotivation|year|\n",
      "+----------+--------------------+-----------------+----+\n",
      "| chemistry|[{Carolyn, 1015, ...|             null|2022|\n",
      "| economics|[{Ben, 1021, \"for...|             null|2022|\n",
      "|literature|[{Annie, 1017, \"f...|             null|2022|\n",
      "|     peace|[{Ales, 1018, \"Th...|             null|2022|\n",
      "|   physics|[{Alain, 1012, \"f...|             null|2022|\n",
      "+----------+--------------------+-----------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.3. `flatten()` , `explode()` and `collect_list()` functions\n",
    "\n",
    "- `collect_list` 函数将相同组中的列值收集到一个列表中"
   ],
   "id": "5ac80b2c2934509c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:08:19.061451Z",
     "start_time": "2025-04-15T15:08:18.858313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import collect_list\n",
    "from pyspark.sql.functions import flatten\n",
    "\n",
    "\n",
    "#collect_list(\"laureates\") 会将 laureates 列的所有值按行聚合到一个列表中。\n",
    "#alias(\"information\") 将结果列重命名为 information\n",
    "collect_df = df2.select(collect_list(\"laureates\").alias(\"information\"))\n",
    "collect_df.show()\n",
    "print(\"_______________________________________________________________\")\n",
    "# use flatten() function to merge all the elements of the inner arrays\n",
    "flattened_df = collect_df.select(flatten(\"information\").alias(\"merged_data\"))\n",
    "flattened_df.show()"
   ],
   "id": "b6bfb5b49bed982a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         information|\n",
      "+--------------------+\n",
      "|[[{Carolyn, 1015,...|\n",
      "+--------------------+\n",
      "\n",
      "_______________________________________________________________\n",
      "+--------------------+\n",
      "|         merged_data|\n",
      "+--------------------+\n",
      "|[{Carolyn, 1015, ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " - `explode()` 函数将数组中的每个元素转换为一行, 并将数组中的每个元素作为新行的值",
   "id": "1a76a628daf6019e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:12:18.855160Z",
     "start_time": "2025-04-15T15:12:18.773845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "df_flattened = (\n",
    "    df2\n",
    "    .withColumn(\"laureates\",explode(col(\"laureates\"))) # Explode the laureates array column into rows\n",
    "    .select(col(\"category\")\n",
    "            , col(\"year\")\n",
    "            , col(\"overallMotivation\")\n",
    "            , col(\"laureates.id\")\n",
    "            , col(\"laureates.firstname\")\n",
    "            , col(\"laureates.surname\")\n",
    "            , col(\"laureates.share\")\n",
    "            , col(\"laureates.motivation\"))) # Use dot notion for columns in the STRUCT field\n",
    "df_flattened.show(5)"
   ],
   "id": "c64f47004615a240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----------------+----+---------+---------+-----+--------------------+\n",
      "| category|year|overallMotivation|  id|firstname|  surname|share|          motivation|\n",
      "+---------+----+-----------------+----+---------+---------+-----+--------------------+\n",
      "|chemistry|2022|             null|1015|  Carolyn| Bertozzi|    3|\"for the developm...|\n",
      "|chemistry|2022|             null|1016|   Morten|   Meldal|    3|\"for the developm...|\n",
      "|chemistry|2022|             null| 743|    Barry|Sharpless|    3|\"for the developm...|\n",
      "|economics|2022|             null|1021|      Ben| Bernanke|    3|\"for research on ...|\n",
      "|economics|2022|             null|1022|  Douglas|  Diamond|    3|\"for research on ...|\n",
      "+---------+----+-----------------+----+---------+---------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2.4. `get_json_object()` and `json_tuple()` functions",
   "id": "e78330784862af46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T14:44:38.115295Z",
     "start_time": "2025-04-15T14:44:35.853947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "\n",
    "# create a DataFrame with a JSON string column\n",
    "df3 = spark.createDataFrame([\n",
    "  (1, '{\"name\": \"Alice\", \"age\": 25}'),\n",
    "  (2, '{\"name\": \"Bob\", \"age\": 30}')\n",
    "], [\"id\", \"json_data\"])\n",
    "\n",
    "# extract the \"name\" field from the JSON string column\n",
    "name_df = df3.select(get_json_object(\"json_data\", \"$.name\").alias(\"name\"))\n",
    "name_df.show()\n",
    "print(\"________________________________\")\n",
    "# cast the extracted value to a string\n",
    "name_str_df = name_df.withColumn(\"name_str\", name_df[\"name\"].cast(StringType()))\n",
    "\n",
    "name_str_df.show()"
   ],
   "id": "1ff05d8b85355bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Alice|\n",
      "|  Bob|\n",
      "+-----+\n",
      "\n",
      "________________________________\n",
      "+-----+--------+\n",
      "| name|name_str|\n",
      "+-----+--------+\n",
      "|Alice|   Alice|\n",
      "|  Bob|     Bob|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Reading Parquet data\n",
    "\n",
    "#### 3.1. What is parquet\n",
    "- Parquet 是一种列式存储格式, 用于存储大规模数据集. Parquet 文件通常以 .parquet 作为文件扩展名, 例如 data.parquet\n",
    "- Parquet 文件具有高效的存储和查询性能, 并且支持多种数据类型和复杂的数据结构"
   ],
   "id": "b870ae125e5e0f72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:30:59.759569Z",
     "start_time": "2025-04-15T15:30:59.421461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read Parquet file into a DataFrame\n",
    "parquet_file_location = \"../data/dataload/recipes.parquet\"\n",
    "df4 = (spark.read.format(\"parquet\")\n",
    "      .load(parquet_file_location))\n",
    "df4.printSchema()"
   ],
   "id": "cd748053519fda90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecipeId: double (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- AuthorId: integer (nullable = true)\n",
      " |-- AuthorName: string (nullable = true)\n",
      " |-- CookTime: string (nullable = true)\n",
      " |-- PrepTime: string (nullable = true)\n",
      " |-- TotalTime: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- RecipeCategory: string (nullable = true)\n",
      " |-- Keywords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- RecipeIngredientQuantities: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- RecipeIngredientParts: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- AggregatedRating: double (nullable = true)\n",
      " |-- ReviewCount: integer (nullable = true)\n",
      " |-- Calories: double (nullable = true)\n",
      " |-- FatContent: double (nullable = true)\n",
      " |-- SaturatedFatContent: double (nullable = true)\n",
      " |-- CholesterolContent: double (nullable = true)\n",
      " |-- SodiumContent: double (nullable = true)\n",
      " |-- CarbohydrateContent: double (nullable = true)\n",
      " |-- FiberContent: double (nullable = true)\n",
      " |-- SugarContent: double (nullable = true)\n",
      " |-- ProteinContent: double (nullable = true)\n",
      " |-- RecipeServings: integer (nullable = true)\n",
      " |-- RecipeYield: string (nullable = true)\n",
      " |-- RecipeInstructions: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Images: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- DatePublished: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:31:28.422057Z",
     "start_time": "2025-04-15T15:31:27.897348Z"
    }
   },
   "cell_type": "code",
   "source": "df4.show(5)",
   "id": "59d7f1186f03e3f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+------------+--------+--------+---------+--------------------+--------------+--------------------+--------------------------+---------------------+----------------+-----------+--------+----------+-------------------+------------------+-------------+-------------------+------------+------------+--------------+--------------+-----------+--------------------+------+-------------+\n",
      "|RecipeId|                Name|  AuthorId|  AuthorName|CookTime|PrepTime|TotalTime|         Description|RecipeCategory|            Keywords|RecipeIngredientQuantities|RecipeIngredientParts|AggregatedRating|ReviewCount|Calories|FatContent|SaturatedFatContent|CholesterolContent|SodiumContent|CarbohydrateContent|FiberContent|SugarContent|ProteinContent|RecipeServings|RecipeYield|  RecipeInstructions|Images|DatePublished|\n",
      "+--------+--------------------+----------+------------+--------+--------+---------+--------------------+--------------+--------------------+--------------------------+---------------------+----------------+-----------+--------+----------+-------------------+------------------+-------------+-------------------+------------+------------+--------------+--------------+-----------+--------------------+------+-------------+\n",
      "|540530.0|Asian Ginger and ...|2002766352|winosity.app|   PT15M|   PT35M|    PT50M|This is a great e...|         Asian|         [< 60 Mins]|      [16, 2, 1⁄2, 1, 1...| [salmon fillets, ...|            null|       null|   234.0|       9.6|                1.6|              51.6|        841.2|               11.3|         1.1|         7.0|          25.4|             4|       null|[Preheat the oven...|    []|   2020-06-26|\n",
      "|540531.0| Awesome Carrot Cake|2002766352|winosity.app|   PT35M|   PT10M|    PT45M|A wonderfully moi...|       Dessert|[Spring, Summer, ...|      [3, 2, 2, 1, 1 1⁄...| [all-purpose flou...|            null|       null|  1072.5|      58.4|                8.2|             124.0|       1017.3|              128.4|         3.2|        77.3|          11.3|             6|       null|[Preheat oven to ...|    []|   2020-06-26|\n",
      "|540532.0|Bacon Wrapped Shr...|2002766352|winosity.app|   PT12M|   PT15M|    PT27M|This bacon wrappe...|        Cheese|[Very Low Carbs, ...|          [24, 6, 2, 4, 8]| [shrimp, bacon, f...|            null|       null|   202.3|      13.9|                5.6|             135.0|        827.6|                2.5|         0.6|         0.8|          16.4|             8|       null|[Preheat oven to ...|    []|   2020-06-26|\n",
      "|540533.0|Baked Cheese in R...|2002766352|winosity.app|   PT15M|    PT5M|    PT20M|Historically spea...|       African|[Spring, Summer, ...|      [8, 1⁄2, 1, 1, 4,...| [butter, parmesan...|            null|       null|   211.4|      15.2|                9.2|              40.3|        628.8|               11.8|         0.7|         1.0|           7.2|             8|       null|[Mix all the ingr...|    []|   2020-06-26|\n",
      "|540534.0|Cheesy Baked Oyst...|2002766352|winosity.app|   PT10M|   PT10M|    PT20M|Baked oysters are...|  Lunch/Snacks|[Spring, Summer, ...|      [16, 5, 1⁄4, 1⁄4,...| [ricotta cheese, ...|            null|       null|   446.2|      28.5|               12.3|             157.3|        663.4|               13.3|         0.5|         0.3|          32.8|             4|       null|[Cut bacon into s...|    []|   2020-06-26|\n",
      "+--------+--------------------+----------+------------+--------+--------+---------+--------------------+--------------+--------------------+--------------------------+---------------------+----------------+-----------+--------+----------+-------------------+------------------+-------------+-------------------+------------+------------+--------------+--------------+-----------+--------------------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.2 Partitioned Parquet files",
   "id": "6a3188ee45998887"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:38:29.366068Z",
     "start_time": "2025-04-15T15:38:28.800859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "partitioned_parquet_file_location = \"../data/dataload/partitioned_recipes\"\n",
    "df_partitioned = (spark.read.format(\"parquet\")\n",
    "                  .load(partitioned_parquet_file_location))\n",
    "df_partitioned.show(5)"
   ],
   "id": "656a8e1c7b459fb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+-------------------+--------+--------+---------+--------------------+--------------+--------------------+--------------------------+---------------------+----------------+-----------+--------+----------+-------------------+------------------+-------------+-------------------+------------+------------+--------------+--------------+-----------+--------------------+--------------------+-------------+\n",
      "|RecipeId|                Name|  AuthorId|         AuthorName|CookTime|PrepTime|TotalTime|         Description|RecipeCategory|            Keywords|RecipeIngredientQuantities|RecipeIngredientParts|AggregatedRating|ReviewCount|Calories|FatContent|SaturatedFatContent|CholesterolContent|SodiumContent|CarbohydrateContent|FiberContent|SugarContent|ProteinContent|RecipeServings|RecipeYield|  RecipeInstructions|              Images|DatePublished|\n",
      "+--------+--------------------+----------+-------------------+--------+--------+---------+--------------------+--------------+--------------------+--------------------------+---------------------+----------------+-----------+--------+----------+-------------------+------------------+-------------+-------------------+------------+------------+--------------+--------------+-----------+--------------------+--------------------+-------------+\n",
      "|539683.0|Buttermilk Quick ...|2002536472|             kim f.|   PT45M|   PT20M|   PT1H5M|Great basic recip...|  Quick Breads|[Breads, Healthy,...|      [2, 1⁄2, 1 1⁄2, 1...| [flour, sugar, ba...|            null|       null|   234.6|       6.9|                4.1|              39.7|        480.0|               38.1|         0.8|        14.1|           5.1|             8|     1 Loaf|[Heat oven to 350...|[https://img.sndi...|   2020-01-01|\n",
      "|539684.0|              Banket|   1052455|         Jena Lewis|   PT20M|    PT9H|  PT9H20M|This recipe is fr...|       Dessert|[Dutch, European,...|      [1, 1⁄4, 4, 1, 1,...| [unsalted butter,...|            null|       null|   102.2|       5.9|                2.9|              19.0|          5.2|               11.5|         0.4|         7.0|           1.3|            96|  12 sticks|[Crust., Mix butt...|[https://img.sndi...|   2020-01-01|\n",
      "|539686.0|Charishma's Homem...|      6357|Charishma_Ramchanda|   PT20M|   PT15M|    PT35M|This amazing humm...|         Beans|[Southwest Asia (...|      [1, 2, 3, 4, 5, 6...| [chickpeas, salt,...|            null|       null|     3.4|       0.0|                0.0|               0.0|          0.4|                0.7|         0.1|         0.0|           0.1|             4|       null|[1. Wash the chic...|                  []|   2020-01-01|\n",
      "|539687.0|Charishma's Sarso...|      6357|Charishma_Ramchanda|   PT30M|   PT30M|     PT1H|This recipe was t...|        Onions|[Vegetable, India...|      [1, 2, 3, 4, 5, 6...| [baking soda, oni...|            null|       null|   247.6|       1.0|                0.2|               0.0|         33.6|               55.7|         6.4|        12.4|          10.5|             4|       null|[1. Boil 2 bunche...|                  []|   2020-01-01|\n",
      "|539688.0|       GOLDEN SUNSET|2001004241|          CLUBFOODY|    null|    PT2M|     PT2M|Orange, hazelnut ...|     Beverages|   [< 15 Mins, Easy]|                 [1, 1, 1]| [Grand Marnier, a...|            null|       null|     0.0|       0.0|                0.0|               0.0|          0.0|                0.0|         0.0|         0.0|           0.0|          null|    1 drink|[In a snifter, co...|[https://img.sndi...|   2020-01-01|\n",
      "+--------+--------------------+----------+-------------------+--------+--------+---------+--------------------+--------------+--------------------+--------------------------+---------------------+----------------+-----------+--------+----------+-------------------+------------------+-------------+-------------------+------------+------------+--------------+--------------+-----------+--------------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:42:21.638717Z",
     "start_time": "2025-04-15T15:42:21.251201Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "26400465974f15fd",
   "outputs": [],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
